{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсинг и сбор даных с сайта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмём сайт https://docdoc.ru, на нём представлена информация о разных врачах.\n",
    "\n",
    "Скачаем эту информацию, чтобы обучить модель предсказывать по регалиям врача стоимость приёма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = \"https://docdoc.ru/search/page/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы послать запрос можно воспользоваться библиотекой requests\n",
    "\n",
    "http://docs.python-requests.org/en/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем запрос по url-у"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим содержимое ответа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим страницу в формате HTML\n",
    "\n",
    "Подробнее про HTML можно почитать здесь: http://htmlbook.ru/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсить это вручную было бы очень сложно, есть специальная библиотека для парсинга HTML страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаём вспомогательный объект для парсинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs4_object = BeautifulSoup(response.text, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs4_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bs4_object.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем использовать разные системы поиска данных (по тегу, по классу и т.д.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_obj = bs4_object.find_all(\"div\", attrs={\"class\": \"doc__info\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.element.ResultSet"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(first_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постепенно, при помощи разных поисков найдём нужные данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-e206dfb62015>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfirst_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"doc__prof\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1805\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m         raise AttributeError(\n\u001b[1;32m-> 1807\u001b[1;33m             \u001b[1;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1808\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find'. You're probably treating a list of items like a single item. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "first_obj.find(\"div\", attrs={\"class\": \"doc__prof\"}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Хирург, Эндоскопист\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tCтаж 23 года\\t\\t\\t\\t\\t\\t\\t/ Врач первой категории, к.м.н.\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = first_obj.find(\"div\", attrs={\"class\": \"doc__prof\"}).text\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Хирург, Эндоскопист', 'Cтаж 23 года', '/ Врач первой категории, к.м.н.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = [p.strip() for p in text.split('\\t') if p != '']\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Хирург, Эндоскопист', 'Cтаж 23 года', '/ Врач первой категории, к.м.н.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Хирург', 'Эндоскопист']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proffesions = [s.strip() for s in parts[0].split(',')]\n",
    "proffesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['Cтаж']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('Стаж', parts[1]))\n",
    "print(re.findall('Cтаж', parts[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cтоп, что???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xd0\\xa1\\xd1\\x82\\xd0\\xb0\\xd0\\xb6'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes('Стаж', 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'C\\xd1\\x82\\xd0\\xb0\\xd0\\xb6'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes('Cтаж', 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В слове \"Стаж\" на сайте английская буква С"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experience = float(re.findall('Cтаж (\\d+)', parts[1])[0])\n",
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_first_category = 'Врач первой категории' in text\n",
    "is_first_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_phd = 'к.м.н.' in text\n",
    "is_phd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = float(re.findall('\\d+', first_obj.find(attrs={'class': 'doc__price-value'}).text)[0])\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Сергейко', 'Анатолий', 'Анатольевич']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_parts = [p for p in re.split('[\\t\\n ]', first_obj.find(attrs={'class': 'doc__name'}).text) if p != '']\n",
    "name_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем итоговую функцию, извлекающую информацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_info(obj):\n",
    "    prof_text = obj.find(\"div\", attrs={\"class\": \"doc__prof\"}).text\n",
    "    prof_parts = [p.strip() for p in prof_text.split('\\t') if p != '']\n",
    "    \n",
    "    proffesions = [s.strip() for s in prof_parts[0].split(',')]\n",
    "    experience_raws = re.findall('Cтаж (\\d+)', prof_text)\n",
    "    if len(experience_raws) > 0:\n",
    "        experience = float(experience_raws[0])\n",
    "    else:\n",
    "        experience = None\n",
    "    \n",
    "    is_first_category = 'Врач первой категории' in prof_text\n",
    "    is_phd = 'к.м.н.' in prof_text\n",
    "    \n",
    "    name_parts = [p for p in re.split('[\\t\\n ]', obj.find(attrs={'class': 'doc__name'}).text) if p != '']\n",
    "    price_raws = re.findall('\\d+', obj.find(attrs={'class': 'doc__price-value'}).text)\n",
    "    if len(price_raws) > 0:\n",
    "        price = float(price_raws[0])\n",
    "    else:\n",
    "        price = None\n",
    "    \n",
    "    return {\n",
    "        'proffesions': proffesions,\n",
    "        'experience': experience,\n",
    "        'is_first_category': is_first_category,\n",
    "        'is_phd': is_phd,\n",
    "        'name_parts': name_parts,\n",
    "        'price': price\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим её работу на скаченной странице"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'proffesions': ['Хирург', 'Эндоскопист'], 'price': 1700.0, 'is_phd': True, 'experience': 23.0, 'is_first_category': True, 'name_parts': ['Сергейко', 'Анатолий', 'Анатольевич']}\n",
      "{'proffesions': ['Кардиолог', 'Терапевт', 'Врач функциональной диагностики'], 'price': 1700.0, 'is_phd': True, 'experience': 19.0, 'is_first_category': False, 'name_parts': ['Сюмакова', 'Светлана', 'Александровна']}\n",
      "{'proffesions': ['Лор (отоларинголог)'], 'price': 2450.0, 'is_phd': True, 'experience': 35.0, 'is_first_category': False, 'name_parts': ['Сергеева', 'Алла', 'Петровна']}\n",
      "{'proffesions': ['Флеболог', 'Хирург', 'Сосудистый хирург'], 'price': 2079.0, 'is_phd': True, 'experience': 28.0, 'is_first_category': False, 'name_parts': ['Бисеков', 'Саламат', 'Хамитович']}\n",
      "{'proffesions': ['Уролог', 'Хирург', 'Проктолог', 'УЗИ-специалист'], 'price': 1300.0, 'is_phd': False, 'experience': 23.0, 'is_first_category': False, 'name_parts': ['Марков', 'Сергей', 'Валерьевич']}\n",
      "{'proffesions': ['Дерматолог', 'Косметолог'], 'price': 1300.0, 'is_phd': False, 'experience': 28.0, 'is_first_category': False, 'name_parts': ['Лемешко', 'Татьяна', 'Анатольевна']}\n",
      "{'proffesions': ['Дерматолог', 'Косметолог', 'Трихолог'], 'price': 1400.0, 'is_phd': False, 'experience': 9.0, 'is_first_category': False, 'name_parts': ['Сливень', 'Елена', 'Сергеевна']}\n",
      "{'proffesions': ['Андролог', 'Венеролог', 'Уролог'], 'price': 1300.0, 'is_phd': False, 'experience': 21.0, 'is_first_category': False, 'name_parts': ['Глазачев', 'Николай', 'Сергеевич']}\n",
      "{'proffesions': ['Гинеколог'], 'price': 1500.0, 'is_phd': False, 'experience': 28.0, 'is_first_category': False, 'name_parts': ['Мемей', 'Светлана', 'Андреевна']}\n",
      "{'proffesions': ['Венеролог', 'Уролог'], 'price': 1500.0, 'is_phd': False, 'experience': 21.0, 'is_first_category': False, 'name_parts': ['Давидьян', 'Валерий', 'Арцвикович']}\n"
     ]
    }
   ],
   "source": [
    "for x in bs4_object.find_all(\"div\", attrs={\"class\": \"doc__info\"}):\n",
    "    print(extract_info(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для каждой страницы с докторами сделаем запрос, будем повторять, пока не получим плохой ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_num 1\n",
      "page_num 2\n",
      "page_num 3\n",
      "page_num 4\n",
      "page_num 5\n",
      "page_num 6\n",
      "page_num 7\n",
      "page_num 8\n",
      "page_num 9\n",
      "page_num 10\n",
      "page_num 11\n",
      "page_num 12\n",
      "page_num 13\n",
      "page_num 14\n",
      "page_num 15\n",
      "page_num 16\n",
      "page_num 17\n",
      "page_num 18\n",
      "page_num 19\n",
      "page_num 20\n",
      "page_num 21\n",
      "page_num 22\n",
      "page_num 23\n",
      "page_num 24\n",
      "page_num 25\n",
      "page_num 26\n",
      "page_num 27\n",
      "page_num 28\n",
      "page_num 29\n",
      "page_num 30\n",
      "page_num 31\n",
      "page_num 32\n",
      "page_num 33\n",
      "page_num 34\n",
      "page_num 35\n",
      "page_num 36\n",
      "page_num 37\n",
      "page_num 38\n",
      "page_num 39\n",
      "page_num 40\n",
      "page_num 41\n",
      "page_num 42\n",
      "page_num 43\n",
      "page_num 44\n",
      "page_num 45\n",
      "page_num 46\n",
      "page_num 47\n",
      "page_num 48\n",
      "page_num 49\n",
      "page_num 50\n",
      "page_num 51\n",
      "page_num 52\n",
      "page_num 53\n",
      "page_num 54\n",
      "page_num 55\n",
      "page_num 56\n",
      "page_num 57\n",
      "page_num 58\n",
      "page_num 59\n",
      "page_num 60\n",
      "page_num 61\n",
      "page_num 62\n",
      "page_num 63\n",
      "page_num 64\n",
      "page_num 65\n",
      "page_num 66\n",
      "page_num 67\n",
      "page_num 68\n",
      "page_num 69\n",
      "page_num 70\n",
      "page_num 71\n",
      "page_num 72\n",
      "page_num 73\n",
      "page_num 74\n",
      "page_num 75\n",
      "page_num 76\n",
      "page_num 77\n",
      "page_num 78\n",
      "page_num 79\n",
      "page_num 80\n",
      "page_num 81\n",
      "page_num 82\n",
      "page_num 83\n",
      "page_num 84\n",
      "page_num 85\n",
      "page_num 86\n",
      "page_num 87\n",
      "page_num 88\n",
      "page_num 89\n",
      "page_num 90\n",
      "page_num 91\n",
      "page_num 92\n",
      "page_num 93\n",
      "page_num 94\n",
      "page_num 95\n",
      "page_num 96\n",
      "page_num 97\n",
      "page_num 98\n",
      "page_num 99\n",
      "page_num 100\n",
      "page_num 101\n",
      "page_num 102\n",
      "page_num 103\n",
      "page_num 104\n",
      "page_num 105\n",
      "page_num 106\n",
      "page_num 107\n",
      "page_num 108\n",
      "page_num 109\n",
      "page_num 110\n",
      "page_num 111\n",
      "page_num 112\n",
      "page_num 113\n",
      "page_num 114\n",
      "page_num 115\n",
      "page_num 116\n",
      "page_num 117\n",
      "page_num 118\n",
      "page_num 119\n",
      "page_num 120\n",
      "page_num 121\n",
      "page_num 122\n",
      "page_num 123\n",
      "page_num 124\n",
      "page_num 125\n",
      "page_num 126\n",
      "page_num 127\n",
      "page_num 128\n",
      "page_num 129\n",
      "page_num 130\n",
      "page_num 131\n",
      "page_num 132\n",
      "page_num 133\n",
      "page_num 134\n",
      "page_num 135\n",
      "page_num 136\n",
      "page_num 137\n",
      "page_num 138\n",
      "page_num 139\n",
      "page_num 140\n",
      "page_num 141\n",
      "page_num 142\n",
      "page_num 143\n",
      "page_num 144\n",
      "page_num 145\n",
      "page_num 146\n",
      "page_num 147\n",
      "page_num 148\n",
      "page_num 149\n",
      "page_num 150\n",
      "page_num 151\n",
      "page_num 152\n",
      "page_num 153\n",
      "page_num 154\n",
      "page_num 155\n",
      "page_num 156\n",
      "page_num 157\n",
      "page_num 158\n",
      "page_num 159\n",
      "page_num 160\n",
      "page_num 161\n",
      "page_num 162\n",
      "page_num 163\n",
      "page_num 164\n",
      "page_num 165\n",
      "page_num 166\n",
      "page_num 167\n",
      "page_num 168\n",
      "page_num 169\n",
      "page_num 170\n",
      "page_num 171\n",
      "page_num 172\n",
      "page_num 173\n",
      "page_num 174\n",
      "page_num 175\n",
      "page_num 176\n",
      "page_num 177\n",
      "page_num 178\n",
      "page_num 179\n",
      "page_num 180\n",
      "page_num 181\n",
      "page_num 182\n",
      "page_num 183\n",
      "page_num 184\n",
      "page_num 185\n",
      "page_num 186\n",
      "page_num 187\n",
      "page_num 188\n",
      "page_num 189\n",
      "page_num 190\n",
      "page_num 191\n",
      "page_num 192\n",
      "page_num 193\n",
      "page_num 194\n",
      "page_num 195\n",
      "page_num 196\n",
      "page_num 197\n",
      "page_num 198\n",
      "page_num 199\n",
      "page_num 200\n",
      "page_num 201\n",
      "page_num 202\n",
      "page_num 203\n",
      "page_num 204\n",
      "page_num 205\n",
      "page_num 206\n",
      "page_num 207\n",
      "page_num 208\n",
      "page_num 209\n",
      "page_num 210\n",
      "page_num 211\n",
      "page_num 212\n",
      "page_num 213\n",
      "page_num 214\n",
      "page_num 215\n",
      "page_num 216\n",
      "page_num 217\n",
      "page_num 218\n",
      "page_num 219\n",
      "page_num 220\n",
      "page_num 221\n",
      "page_num 222\n",
      "page_num 223\n",
      "page_num 224\n",
      "page_num 225\n",
      "page_num 226\n",
      "page_num 227\n",
      "page_num 228\n",
      "page_num 229\n",
      "page_num 230\n",
      "page_num 231\n",
      "page_num 232\n",
      "page_num 233\n",
      "page_num 234\n",
      "page_num 235\n",
      "page_num 236\n",
      "page_num 237\n",
      "page_num 238\n",
      "page_num 239\n",
      "page_num 240\n",
      "page_num 241\n",
      "page_num 242\n",
      "page_num 243\n",
      "page_num 244\n",
      "page_num 245\n",
      "page_num 246\n",
      "page_num 247\n",
      "page_num 248\n",
      "page_num 249\n",
      "page_num 250\n",
      "page_num 251\n",
      "page_num 252\n",
      "page_num 253\n",
      "page_num 254\n",
      "page_num 255\n",
      "page_num 256\n",
      "page_num 257\n",
      "page_num 258\n",
      "page_num 259\n",
      "page_num 260\n",
      "page_num 261\n",
      "page_num 262\n",
      "page_num 263\n",
      "page_num 264\n",
      "page_num 265\n",
      "page_num 266\n",
      "page_num 267\n",
      "page_num 268\n",
      "page_num 269\n",
      "page_num 270\n",
      "page_num 271\n",
      "page_num 272\n",
      "page_num 273\n",
      "page_num 274\n",
      "page_num 275\n",
      "page_num 276\n",
      "page_num 277\n",
      "page_num 278\n",
      "page_num 279\n",
      "page_num 280\n",
      "page_num 281\n",
      "page_num 282\n",
      "page_num 283\n",
      "page_num 284\n",
      "page_num 285\n",
      "page_num 286\n",
      "page_num 287\n",
      "page_num 288\n",
      "page_num 289\n",
      "page_num 290\n",
      "page_num 291\n",
      "page_num 292\n",
      "page_num 293\n",
      "page_num 294\n",
      "page_num 295\n",
      "page_num 296\n",
      "page_num 297\n",
      "page_num 298\n",
      "page_num 299\n",
      "page_num 300\n",
      "page_num 301\n",
      "page_num 302\n",
      "page_num 303\n",
      "page_num 304\n",
      "page_num 305\n",
      "page_num 306\n",
      "page_num 307\n",
      "page_num 308\n",
      "page_num 309\n",
      "page_num 310\n",
      "page_num 311\n",
      "page_num 312\n",
      "page_num 313\n",
      "page_num 314\n",
      "page_num 315\n",
      "page_num 316\n",
      "page_num 317\n",
      "page_num 318\n",
      "page_num 319\n",
      "page_num 320\n",
      "page_num 321\n",
      "page_num 322\n",
      "page_num 323\n",
      "page_num 324\n",
      "page_num 325\n",
      "page_num 326\n",
      "page_num 327\n",
      "page_num 328\n",
      "page_num 329\n",
      "page_num 330\n",
      "page_num 331\n",
      "page_num 332\n",
      "page_num 333\n",
      "page_num 334\n",
      "page_num 335\n",
      "page_num 336\n",
      "page_num 337\n",
      "page_num 338\n",
      "page_num 339\n",
      "page_num 340\n",
      "page_num 341\n",
      "page_num 342\n",
      "page_num 343\n",
      "page_num 344\n",
      "page_num 345\n",
      "page_num 346\n",
      "page_num 347\n",
      "page_num 348\n",
      "page_num 349\n",
      "page_num 350\n",
      "page_num 351\n",
      "page_num 352\n",
      "page_num 353\n",
      "page_num 354\n",
      "page_num 355\n",
      "page_num 356\n",
      "page_num 357\n",
      "page_num 358\n",
      "page_num 359\n",
      "page_num 360\n",
      "page_num 361\n",
      "page_num 362\n",
      "page_num 363\n",
      "page_num 364\n",
      "page_num 365\n",
      "page_num 366\n",
      "page_num 367\n",
      "page_num 368\n",
      "page_num 369\n",
      "page_num 370\n",
      "page_num 371\n",
      "page_num 372\n",
      "page_num 373\n",
      "page_num 374\n",
      "page_num 375\n",
      "page_num 376\n",
      "page_num 377\n",
      "page_num 378\n",
      "page_num 379\n",
      "page_num 380\n",
      "page_num 381\n",
      "page_num 382\n",
      "page_num 383\n",
      "page_num 384\n",
      "page_num 385\n",
      "page_num 386\n",
      "page_num 387\n",
      "page_num 388\n",
      "page_num 389\n",
      "page_num 390\n",
      "page_num 391\n",
      "page_num 392\n",
      "page_num 393\n",
      "page_num 394\n",
      "page_num 395\n",
      "page_num 396\n",
      "page_num 397\n",
      "page_num 398\n",
      "page_num 399\n",
      "page_num 400\n",
      "page_num 401\n",
      "page_num 402\n",
      "page_num 403\n",
      "page_num 404\n",
      "page_num 405\n",
      "page_num 406\n",
      "page_num 407\n",
      "page_num 408\n",
      "page_num 409\n",
      "page_num 410\n",
      "page_num 411\n",
      "page_num 412\n",
      "page_num 413\n",
      "page_num 414\n",
      "page_num 415\n",
      "page_num 416\n",
      "page_num 417\n",
      "page_num 418\n",
      "page_num 419\n",
      "page_num 420\n",
      "page_num 421\n",
      "page_num 422\n",
      "page_num 423\n",
      "page_num 424\n",
      "page_num 425\n",
      "page_num 426\n",
      "page_num 427\n",
      "page_num 428\n",
      "page_num 429\n",
      "page_num 430\n",
      "page_num 431\n",
      "page_num 432\n",
      "page_num 433\n",
      "page_num 434\n",
      "page_num 435\n",
      "page_num 436\n",
      "page_num 437\n",
      "page_num 438\n",
      "page_num 439\n",
      "page_num 440\n",
      "page_num 441\n",
      "page_num 442\n",
      "page_num 443\n",
      "page_num 444\n",
      "page_num 445\n",
      "page_num 446\n",
      "page_num 447\n",
      "page_num 448\n",
      "page_num 449\n",
      "page_num 450\n",
      "page_num 451\n",
      "page_num 452\n",
      "page_num 453\n",
      "page_num 454\n",
      "page_num 455\n",
      "page_num 456\n",
      "page_num 457\n",
      "page_num 458\n",
      "page_num 459\n",
      "page_num 460\n",
      "page_num 461\n",
      "page_num 462\n",
      "page_num 463\n",
      "page_num 464\n",
      "page_num 465\n",
      "page_num 466\n",
      "page_num 467\n",
      "page_num 468\n",
      "page_num 469\n",
      "page_num 470\n",
      "page_num 471\n",
      "page_num 472\n",
      "page_num 473\n",
      "page_num 474\n",
      "page_num 475\n",
      "page_num 476\n",
      "page_num 477\n",
      "page_num 478\n",
      "page_num 479\n",
      "page_num 480\n",
      "page_num 481\n",
      "page_num 482\n",
      "page_num 483\n",
      "page_num 484\n",
      "page_num 485\n",
      "page_num 486\n",
      "page_num 487\n",
      "page_num 488\n",
      "page_num 489\n",
      "page_num 490\n",
      "page_num 491\n",
      "page_num 492\n",
      "page_num 493\n",
      "page_num 494\n",
      "page_num 495\n",
      "page_num 496\n",
      "page_num 497\n",
      "page_num 498\n",
      "page_num 499\n",
      "page_num 500\n",
      "page_num 501\n",
      "page_num 502\n",
      "page_num 503\n",
      "page_num 504\n",
      "page_num 505\n",
      "page_num 506\n",
      "page_num 507\n",
      "page_num 508\n",
      "page_num 509\n",
      "CPU times: user 2min 53s, sys: 996 ms, total: 2min 54s\n",
      "Wall time: 16min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "page_num = 0\n",
    "doctors = []\n",
    "\n",
    "while True:\n",
    "    page_num += 1 \n",
    "    print ('page_num {}'.format(page_num))\n",
    "    url = \"https://docdoc.ru/search/page/{}\".format(page_num)\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        break\n",
    "    bs4_object = BeautifulSoup(response.text, \"html5lib\")\n",
    "    for d in bs4_object.find_all(\"div\", attrs={\"class\": \"doc__info\"}):\n",
    "        doctors.append(extract_info(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cохраним данные на диск в формате JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('doctors.json', 'w') as f:\n",
    "    json.dump(doctors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть более гиибкая библиотека scrapy, позволяющая более эффективно парсить сайте и имеющая встроенные дополнительные улучшения (задержка между запросами, логгирования, асинхронные запросы) для более гибкой настройки.\n",
    "\n",
    "Если нужно собирать данные в промышленных мастабах, то лучше пользоваться scrapy, если задача небольшая, то проще использовать requests + BeautifulSoup.\n",
    "\n",
    "Документация scrapy: https://docs.scrapy.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class DoctorsSpider(scrapy.Spider):\n",
    "    name = 'doctors'\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.domain = 'https://docdoc.ru' # нужно указать на какой домен будут запросы\n",
    "        self.download_delay = 3. # задержка между запросами\n",
    "        \n",
    "    def start_requests(self):\n",
    "        for page in range(1, 51): # сделаем запросы только на 50 страниц\n",
    "            yield scrapy.Request(\"https://docdoc.ru/search/page/{}\".format(page))\n",
    "            \n",
    "    def parse(self, response):\n",
    "        # вообще у scrapy есть свой парсинг с поиском, но для простоты (и чтобы меньше кода менять) можно и так\n",
    "        bs4_object = BeautifulSoup(response.body, \"html5lib\")\n",
    "        for x in bs4_object.find_all(\"div\", attrs={\"class\": \"doc__info\"}):\n",
    "            yield extract_info(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-05 22:18:39 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 'INFO', 'FEED_URI': 'doctors_scrapy.json', 'FEED_FORMAT': 'json'}\n",
      "2018-02-05 22:18:39 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2018-02-05 22:18:39 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2018-02-05 22:18:39 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2018-02-05 22:18:39 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2018-02-05 22:18:39 [scrapy.core.engine] INFO: Spider opened\n",
      "2018-02-05 22:18:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2018-02-05 22:19:39 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 18 pages/min), scraped 180 items (at 180 items/min)\n",
      "2018-02-05 22:20:39 [scrapy.extensions.logstats] INFO: Crawled 34 pages (at 16 pages/min), scraped 331 items (at 151 items/min)\n",
      "2018-02-05 22:21:32 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2018-02-05 22:21:32 [scrapy.extensions.feedexport] INFO: Stored json feed (500 items) in: doctors_scrapy.json\n",
      "2018-02-05 22:21:32 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 17786,\n",
      " 'downloader/request_count': 50,\n",
      " 'downloader/request_method_count/GET': 50,\n",
      " 'downloader/response_bytes': 2311727,\n",
      " 'downloader/response_count': 50,\n",
      " 'downloader/response_status_count/200': 50,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2018, 2, 5, 19, 21, 32, 988313),\n",
      " 'item_scraped_count': 500,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 139927552,\n",
      " 'memusage/startup': 139927552,\n",
      " 'response_received_count': 50,\n",
      " 'scheduler/dequeued': 50,\n",
      " 'scheduler/dequeued/memory': 50,\n",
      " 'scheduler/enqueued': 50,\n",
      " 'scheduler/enqueued/memory': 50,\n",
      " 'start_time': datetime.datetime(2018, 2, 5, 19, 18, 39, 751647)}\n",
      "2018-02-05 22:21:32 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.9 s, sys: 88 ms, total: 18 s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from twisted.internet import reactor\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "configure_logging()\n",
    "runner = CrawlerRunner({'FEED_FORMAT': 'json', 'FEED_URI': 'doctors_scrapy.json', 'LOG_LEVEL': 'INFO'})\n",
    "d = runner.crawl(DoctorsSpider)\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
